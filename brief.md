# Language to Rewards for Robotic Skill Synthesis

由Google DeepMind团队发表。以下是文章的主要内容总结：

### 摘要:

文章探讨了大型语言模型（LLMs）的进步及其在机器人控制中的应用。LLMs已经在上下文学习中展示了获取多种能力的进展，包括逻辑推理和代码编写。但是，由于低级机器人动作的硬件依赖性及其在LLM训练语料库中的代表性不足，将LLMs应用于机器人学是具有挑战性的。本文介绍了一个利用LLMs定义可以优化以完成各种机器人任务的奖励参数的范例。这种方法弥合了高级语言指令和低级机器人动作之间的鸿沟，允许用户交互地观察结果并提供反馈。

### 方法论:

所提议的系统由基于预训练LLMs的**奖励翻译器**和基于MuJoCo MPC的**动作控制器**组成。奖励翻译器理解用户意图并调整所有奖励参数，而动作控制器则采取生成的奖励并交互地优化最佳动作序列。奖励翻译器基于LLMs将用户交互映射到与所需机器人动作相对应的奖励功能。语言到奖励的转换被分解为两个阶段：动作描述和奖励编码任务。

### 结果:

所提议的方法在17个模拟机器人任务上进行了系统评估，结果显示它可以可靠地处理设计任务的90%。与使用原始技能作为接口的基线相比，该方法实现了50%的任务。该方法进一步在真实的机器人臂上进行了验证，展示了通过交互系统产生的复杂操纵技能。

# PROG PROMPT: 使用大型语言模型生成位于特定环境中的机器人任务计划

### 摘要：

本文探讨了在机器人任务规划中使用大型语言模型（LLMs）的方法。LLMs能够评分潜在的下一步动作，并直接生成动作序列，只需自然语言的指令。然而，现有方法要么需要枚举所有可能的下一步，要么生成可能包含不可行动作的自由格式文本。作者提出了一种程序化的LLM提示结构，该结构能够在不同的环境、机器人能力和任务中生成功能性的计划。该方法涉及使用程序化的规范提示LLM，规定环境中可用的动作和对象，以及可以执行的示例程序。

### 引言：

日常家庭任务需要对世界的常识理解和对当前环境的实际知识。通过大型语料库的训练，LLMs已被利用来在机器人任务规划的背景下生成可能的动作计划。然而，这些模型缺乏状态反馈和实际意识，经常提出在给定上下文中不可能的动作。本文通过使用名为PROG PROMPT的提示方案引入了基于LLM的机器人任务规划中的实际意识，该方案利用编程语言结构，并为LLM提供Pythonic程序头，该头导入可用动作及其预期参数，显示环境对象列表，然后定义函数，这些函数的主体是在对象上操作的动作序列。这种方法通过断言前提条件的计划、例如在尝试打开冰箱之前靠近它，并通过恢复动作对失败的断言作出响应，将实际状态反馈纳入环境中。

### 方法论：

* **将机器人计划表示为Pythonic函数：** 机器人计划被表示为Pythonic程序，计划函数由动作原语的API调用、注释以总结动作和断言以跟踪执行组成。注释帮助将高级任务分解为逻辑子任务并通知LLM即将到来的动作的目标。断言提供了一个环境反馈机制，以确保前提条件得到满足，并在它们不满足时启用错误恢复。
* **构建编程语言提示：** 通过提示构建向LLM提供有关环境和原始动作的信息。这种方法确保生成的计划通常包含代理人可以采取的动作和环境中可用的对象。PROG PROMPT还包括一些示例任务—完全可执行的程序计划。

### 图表与示例：

* **图1：** 描述了PROG PROMPT如何利用LLMs在世界知识和编程语言理解方面的优势来生成可以直接执行的位于特定环境中的任务计划。
* **图2和图3：** 描述了PROG PROMPT的结构和示例，展示了如何使用Pythonic计划执行任务。

### 背景和相关工作：

背景部分讨论了机器人任务规划中的挑战以及如何在预定义的领域中进行搜索。本文还讨论了LLMs在任务规划中的作用，以及如何训练它们以展示多任务泛化。

# RT-2： Vision-Language-Action Models Transfer Web Knowledge to Robot Control

由Google DeepMind的团队撰写，发表于2023年7月28日。

### 概述：

这篇文章主要介绍了RT-2模型，这是一种利用网络知识来进行机器人控制的视觉-语言-动作模型。RT-2模型通过微调大型视觉-语言模型（VLMs），这些模型是在网络规模数据上进行训练的，直接作为可泛化和具有语义意识的机器人策略。这种方法结合了从机器人数据中学到的物理动作和从网络数据中学到的图像和文本解释能力。

### 主要内容和贡献：

* **模型结构** ：RT-2模型将机器人动作表示为另一种语言，可以转换为文本令牌，并与Internet规模的视觉-语言数据集一起训练。在推理期间，文本令牌被解令牌化成机器人动作，实现闭环控制。
* **实时推理** ：由于现代VLMs的大小可以达到数百亿参数，因此开发了一种协议，允许通过在多TPU云服务中部署模型并通过网络查询此服务来在机器人上运行RT-2模型。
* **实验** ：实验侧重于RT-2的实际应用和新能力的出现，展示了RT-2模型在对象、场景和指令的泛化方面的显著改进，并展示了从网络规模视觉-语言预训练中继承的一系列新能力。

### 实验和应用：

* **Co-Fine-Tuning** ：实验显示，与原始网络数据一起对机器人数据进行协同微调，而不是仅在机器人数据上进行微调，可以改善机器人性能。
* **应用** ：模型能够重新利用从机器人数据中学到的拾取和放置技能，将对象放置在语义指示的位置附近，例如特定的数字或图标，尽管这些线索在机器人数据中并不存在。

### 结论：

这篇文章的主要贡献是RT-2，一种模型家族，它通过微调在网络规模数据上训练的大型视觉-语言模型，直接作为可泛化和语义感知的机器人策略。实验表明，RT-2模型能够显著改善对对象、场景和指令的泛化，并展示了从网络规模视觉-语言预训练中继承的一系列新能力。

# Instruct2Act：Mapping Multi-modality Instructions to Robotic Actions with Large Language Model

### 主要目标和挑战：

文章的主要目标是开发一种类似于ChatGPT的机器人系统，该系统支持机器人操控、视觉目标达成和视觉推理。开发能够在动态环境中执行复杂任务的通用机器人系统是机器人研究中的一项重大挑战。这样的系统必须具备感知周围环境、选择相关机器人技能并相应地对其进行排序以实现长期目标的能力。

### 方法和框架：

* **Instruct2Act框架** ：该框架允许机器人基于用户的指令和由顶视图相机捕获的观察图像执行一系列动作。它旨在改变环境中的对象状态以匹配指令描述中的配置。
* **执行过程** ：框架使用LLM生成可执行代码，该代码调用视觉基础模型的API来识别环境。通过识别到的对象语义信息，我们生成可能的动作，这些动作被发送到低级控制器以执行任务。
* **无需微调** ：该框架使用视觉基础模型作为可以通过API调用的模块化工具，无需进行微调，从而消除了数据收集和训练成本的需要。

### 实验和效果：

* 文章通过在不同场景中的桌面操控领域中的机器人任务上评估它来验证了该方法的实用性和效率。
* 该方法的零射击（zero-shot）性能在多个任务中均优于许多最先进的基于学习的策略。

### 代码和应用：

* 该框架的代码已经在[GitHub](https://github.com/OpenGVLab/Instruct2Act)上公开，作为具有多模态输入的高级机器人指令任务的稳健基准。

### 结论：

该文章提出了一种新颖的框架，该框架结合了大型语言模型和多模态基础模型，以开发能够理解和执行高级指令的通用机器人操控系统。该框架具有实用性和效率，并且其代码已经公开，为相关研究提供了一个基准。

# TidyBot: Personalized Robot Assistance with Large Language Models

### 摘要：

文章探讨了如何实现个性化的机器人清理助手，该助手能够学习用户的偏好并将这些偏好应用于未来的场景。研究人员提出了一种名为TidyBot的系统，该系统能够通过捡起并整理物品来清理房间。挑战在于确定每个物品的正确位置，因为人们的偏好可能会因个人品味或文化背景而异。研究人员展示了机器人如何结合基于语言的规划和感知以及大型语言模型（LLMs）的few-shot总结能力，来推断出可广泛应用于未来互动的用户偏好。这种方法实现了快速适应，并在基准数据集上达到了91.2%的准确率。实际测试中，TidyBot成功地整理了85.0%的物品。

### 方法：

* **个性化容器选择** ：系统首先接收反映用户个人偏好的一些物品放置示例。LLM将这些示例总结为一般规则，并使用总结来确定新物品的放置位置。
* **实际部署** ：研究人员在一个实际的移动操控系统上部署了这种方法，用于家庭清理。

### 结果：

* 在基准数据集上，该方法表现出良好的泛化能力，准确率达到91.2%。
* 在实际测试场景中，TidyBot正确地整理了85.0%的物品。

### 贡献：

* 提出了文本总结与LLMs提供了机器人学中的泛化手段的观点。
* 发布了一个公开的基准数据集，用于评估容器选择偏好的泛化。
* 在实际的移动操控系统上实施和评估了该方法。

### 相关工作：

* 文章讨论了家庭清理、物品排序和LLMs在机器人学中的应用等相关工作。
* 与其他工作相比，本文展示了LLMs的总结能力如何实现机器人学中的泛化。

### 实际应用：

* 研究人员在实际的机器人移动操控系统上调查了这种方法，该系统被称为TidyBot。
* 在实际测试场景中，TidyBot成功地整理了85.0%的物品。

### 项目页面：

* 更多的补充材料、基准数据集、代码和TidyBot实际操作的定性视频可以在[项目页面](https://tidybot.cs.princeton.edu/)上找到。

# PaLM-E: An Embodied Multimodal Language Model

主要探讨了一种集成了大型语言模型（LLM）和视觉变换器（ViT）的多模态语言模型，名为PaLM-E。

### 摘要：

PaLM-E模型是一种多模态语言模型，它集成了具有562B参数的PaLM LLM和22B参数的ViT。该模型能够进行零射击（zero-shot）多模态链式思考（CoT）推理，展现了在多个任务上的多样化能力，包括视觉条件下的笑话生成、OCR免费的数学推理、多图像推理等。该模型在OK-VQA基准测试中表现出色，并且能够在机器人视觉感知、对话和计划等方面展现出多种能力。

### 主要内容和贡献：

* **模型结构** ：PaLM-E模型集成了PaLM LLM和ViT，成为目前报道中最大的视觉-语言模型。
* **多模态能力** ：该模型展现了多种能力，包括零射击多模态链式思考推理、少射击提示、OCR免费的数学推理、多图像推理等。
* **实验结果** ：PaLM-E-562B在OK-VQA基准测试中实现了最先进的性能，无需依赖任务特定的微调。
* **机器人应用** ：模型展示了在机器人视觉感知、对话和计划等方面的应用能力。
* **物理推理和空间定位** ：模型能够进行物理预测和空间定位，例如预测机器人的下一步动作和识别物体的特定特征。

### 结论：

PaLM-E模型通过多任务训练展现了在多个任务上的高效性和泛化能力，特别是在机器人任务上表现出高数据效率。该模型通过集成大型语言模型和视觉变换器，实现了一系列多模态能力，为多模态学习和机器人学提供了新的可能性和应用方向。

# RT-1: A Robotics Transformer for Vision-Language Robotic Control

主要探讨了一种名为RT-1的机器人学变换器模型，该模型用于视觉-语言机器人控制。以下是对这篇文章的详细总结：

### 摘要：

文章提出了一种名为RT-1的机器人变换器模型，该模型能够吸收大量数据并有效地推广。RT-1模型采用图像和自然语言指令作为输入，并在每个时间步为机器人输出一个动作。该模型经过大量实际训练和评估，展现了令人印象深刻的推广性、稳健性和多样性学习能力。

### 主要内容和贡献：

* **模型结构** ：RT-1模型结合了EfficientNet、TokenLearner和Transformer，以实现高效且大容量的架构。该模型能够以3Hz的频率输出动作，尽管其具有3500万个参数。
* **数据集** ：文章使用了一个大规模、多样化的数据集，该数据集包含了约130,000个示范和超过700个不同的任务指令，这些数据是在17个月的时间里通过13个机器人收集的。
* **实验结果** ：RT-1模型在训练指令上的成功率达到了97%，并且能够比下一个最佳基线更好地推广到新任务、干扰物和背景，分别提高了25%、36%和18%。
* **多任务学习** ：RT-1模型展现了在多任务学习中的高效性和泛化能力，特别是在机器人任务上表现出高数据效率。
* **实时控制** ：RT-1模型能够实现实时控制，满足实际机器人在实时环境中运行的需求。

### 方法和技术：

* **Imitation Learning** ：该模型使用模仿学习方法在演示数据集上训练策略，通过行为克隆优化π，最小化给定图像和语言指令的动作的负对数似然。
* **Action Tokenization** ：每个动作维度在RT-1中被离散化为256个bins。动作维度包括臂部运动的七个变量、基座运动的三个变量和一个用于切换三种模式的离散变量。
* **Inference Speed** ：RT-1模型具有快速且一致的推理速度，这是实际机器人在实时环境中运行的独特要求。

### 结论：

RT-1模型通过大规模、实际的训练和评估展示了显著的推广和稳健性。该模型不仅能够执行非常长期的任务，并且可以整合来自模拟或其他机器人类型的数据，保持对原始任务的性能并改善对新场景的推广。

# PROGPROMPT: Generating Situated Robot Task Plans using Large Language Models

这篇文章主要探讨了一种新的方法，名为PROGPROMPT，该方法使用大型语言模型（LLM）来生成可以在虚拟和物理环境中执行的程序。文章中详细描述了该方法的实现和评估，并展示了其在虚拟家庭环境和实际机器人上的应用。

### 主要内容：

1. **背景与相关工作：**
   * 文章提到了任务规划的背景，指出大多数机器人在预定义的领域中使用搜索进行高级规划。无条件搜索在具有许多可行动作和对象的环境中可能难以扩展。
2. **方法：**
   * PROGPROMPT方法使用了大型语言模型（如GPT-3和CODEX）来生成程序，这些程序可以在虚拟家庭环境中执行，也可以控制实际的机器人。
   * 该方法使用了特定的提示格式，包括导入语句、对象列表和示例任务，以生成计划。
   * 生成的计划可以在虚拟代理或物理机器人系统中执行，每个动作命令都会在环境中执行。
3. **实验：**
   * 文章中进行了模拟实验和真实机器人实验来评估该方法。
   * 在模拟实验中，使用了Virtual Home（VH）环境，这是一个用于典型家庭活动的确定性模拟平台。实验涉及70个家庭任务，例如“微波炉烹饪三文鱼”。
   * 在真实机器人实验中，使用了Franka-Emika Panda机器人，并实施了一个pick-and-place策略。
4. **评估与结果：**
   * 文章展示了PROGPROMPT在Virtual Home环境中生成的程序的评估结果，与其他方法相比，PROGPROMPT表现出显著的优势。
   * 文章中的表格展示了不同的LLM背景下PROGPROMPT的性能，以及与基线方法的比较。

### 结论：

这篇文章提出了一种新颖的方法，利用大型语言模型生成可执行程序，这些程序可以在虚拟环境和实际机器人中执行。实验结果表明，该方法在生成程序方面具有优越性能，表现出了实际应用的潜力。

# Socratic Models: Composing Zero-Shot Multimodal Reasoning with Language

### 摘要：

文章提出了一种新的模型，名为“Socratic Models”，该模型能够进行零射击多模态推理。该模型结合了自然语言处理和计算机视觉，以解决需要多模态理解的问题。文章中详细描述了该模型的结构、训练方法和实验评估，并展示了其在多个任务上的应用。

### 主要内容和贡献：

* **模型结构** ：Socratic Models结合了大型语言模型和视觉模型，以实现自然语言和视觉信息的联合理解。模型能够处理自然语言指令和图像输入，并生成相应的输出。
* **零射击推理** ：该模型能够进行零射击推理，即在没有特定任务的训练数据的情况下，解决特定任务的问题。
* **多模态任务** ：文章展示了该模型在多个需要多模态理解的任务上的应用，例如视觉问答、物体检测和语言导向的任务规划等。
* **实验评估** ：文章中进行了一系列实验，以评估模型在不同任务上的性能。实验结果表明，该模型在多个基准测试中表现优异。

### 方法和技术：

* **模型训练** ：模型采用了大量的多模态数据进行训练，包括自然语言文本和图像数据。训练过程中，模型学习了语言和视觉信息的联合表示。
* **任务适应** ：模型能够适应不同的多模态任务，通过处理自然语言指令和图像输入，生成任务相关的输出。

### 结论：

Socratic Models通过结合大型语言模型和视觉模型，实现了零射击多模态推理的能力。该模型在多个需要多模态理解的任务上表现出色，展现了在实际应用中的潜力。
