# Language to Rewards for Robotic Skill Synthesis

由Google DeepMind团队发表。以下是文章的主要内容总结：

### 摘要:

文章探讨了大型语言模型（LLMs）的进步及其在机器人控制中的应用。LLMs已经在上下文学习中展示了获取多种能力的进展，包括逻辑推理和代码编写。但是，由于低级机器人动作的硬件依赖性及其在LLM训练语料库中的代表性不足，将LLMs应用于机器人学是具有挑战性的。本文介绍了一个利用LLMs定义可以优化以完成各种机器人任务的奖励参数的范例。这种方法弥合了高级语言指令和低级机器人动作之间的鸿沟，允许用户交互地观察结果并提供反馈。

### 方法论:

所提议的系统由基于预训练LLMs的**奖励翻译器**和基于MuJoCo MPC的**动作控制器**组成。奖励翻译器理解用户意图并调整所有奖励参数，而动作控制器则采取生成的奖励并交互地优化最佳动作序列。奖励翻译器基于LLMs将用户交互映射到与所需机器人动作相对应的奖励功能。语言到奖励的转换被分解为两个阶段：动作描述和奖励编码任务。

### 结果:

所提议的方法在17个模拟机器人任务上进行了系统评估，结果显示它可以可靠地处理设计任务的90%。与使用原始技能作为接口的基线相比，该方法实现了50%的任务。该方法进一步在真实的机器人臂上进行了验证，展示了通过交互系统产生的复杂操纵技能。

# PROG PROMPT: 使用大型语言模型生成位于特定环境中的机器人任务计划

### 摘要：

本文探讨了在机器人任务规划中使用大型语言模型（LLMs）的方法。LLMs能够评分潜在的下一步动作，并直接生成动作序列，只需自然语言的指令。然而，现有方法要么需要枚举所有可能的下一步，要么生成可能包含不可行动作的自由格式文本。作者提出了一种程序化的LLM提示结构，该结构能够在不同的环境、机器人能力和任务中生成功能性的计划。该方法涉及使用程序化的规范提示LLM，规定环境中可用的动作和对象，以及可以执行的示例程序。

### 引言：

日常家庭任务需要对世界的常识理解和对当前环境的实际知识。通过大型语料库的训练，LLMs已被利用来在机器人任务规划的背景下生成可能的动作计划。然而，这些模型缺乏状态反馈和实际意识，经常提出在给定上下文中不可能的动作。本文通过使用名为PROG PROMPT的提示方案引入了基于LLM的机器人任务规划中的实际意识，该方案利用编程语言结构，并为LLM提供Pythonic程序头，该头导入可用动作及其预期参数，显示环境对象列表，然后定义函数，这些函数的主体是在对象上操作的动作序列。这种方法通过断言前提条件的计划、例如在尝试打开冰箱之前靠近它，并通过恢复动作对失败的断言作出响应，将实际状态反馈纳入环境中。

### 方法论：

* **将机器人计划表示为Pythonic函数：** 机器人计划被表示为Pythonic程序，计划函数由动作原语的API调用、注释以总结动作和断言以跟踪执行组成。注释帮助将高级任务分解为逻辑子任务并通知LLM即将到来的动作的目标。断言提供了一个环境反馈机制，以确保前提条件得到满足，并在它们不满足时启用错误恢复。
* **构建编程语言提示：** 通过提示构建向LLM提供有关环境和原始动作的信息。这种方法确保生成的计划通常包含代理人可以采取的动作和环境中可用的对象。PROG PROMPT还包括一些示例任务—完全可执行的程序计划。

### 图表与示例：

* **图1：** 描述了PROG PROMPT如何利用LLMs在世界知识和编程语言理解方面的优势来生成可以直接执行的位于特定环境中的任务计划。
* **图2和图3：** 描述了PROG PROMPT的结构和示例，展示了如何使用Pythonic计划执行任务。

### 背景和相关工作：

背景部分讨论了机器人任务规划中的挑战以及如何在预定义的领域中进行搜索。本文还讨论了LLMs在任务规划中的作用，以及如何训练它们以展示多任务泛化。
