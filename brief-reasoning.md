# Language to Rewards for Robotic Skill Synthesis

由Google DeepMind团队发表。以下是文章的主要内容总结：

### 摘要:

文章探讨了大型语言模型（LLMs）的进步及其在机器人控制中的应用。LLMs已经在上下文学习中展示了获取多种能力的进展，包括逻辑推理和代码编写。但是，由于低级机器人动作的硬件依赖性及其在LLM训练语料库中的代表性不足，将LLMs应用于机器人学是具有挑战性的。本文介绍了一个利用LLMs定义可以优化以完成各种机器人任务的奖励参数的范例。这种方法弥合了高级语言指令和低级机器人动作之间的鸿沟，允许用户交互地观察结果并提供反馈。

### 方法论:

所提议的系统由基于预训练LLMs的**奖励翻译器**和基于MuJoCo MPC的**动作控制器**组成。奖励翻译器理解用户意图并调整所有奖励参数，而动作控制器则采取生成的奖励并交互地优化最佳动作序列。奖励翻译器基于LLMs将用户交互映射到与所需机器人动作相对应的奖励功能。语言到奖励的转换被分解为两个阶段：动作描述和奖励编码任务。

### 结果:

所提议的方法在17个模拟机器人任务上进行了系统评估，结果显示它可以可靠地处理设计任务的90%。与使用原始技能作为接口的基线相比，该方法实现了50%的任务。该方法进一步在真实的机器人臂上进行了验证，展示了通过交互系统产生的复杂操纵技能。

# PROG PROMPT: 使用大型语言模型生成位于特定环境中的机器人任务计划

### 摘要：

本文探讨了在机器人任务规划中使用大型语言模型（LLMs）的方法。LLMs能够评分潜在的下一步动作，并直接生成动作序列，只需自然语言的指令。然而，现有方法要么需要枚举所有可能的下一步，要么生成可能包含不可行动作的自由格式文本。作者提出了一种程序化的LLM提示结构，该结构能够在不同的环境、机器人能力和任务中生成功能性的计划。该方法涉及使用程序化的规范提示LLM，规定环境中可用的动作和对象，以及可以执行的示例程序。

### 引言：

日常家庭任务需要对世界的常识理解和对当前环境的实际知识。通过大型语料库的训练，LLMs已被利用来在机器人任务规划的背景下生成可能的动作计划。然而，这些模型缺乏状态反馈和实际意识，经常提出在给定上下文中不可能的动作。本文通过使用名为PROG PROMPT的提示方案引入了基于LLM的机器人任务规划中的实际意识，该方案利用编程语言结构，并为LLM提供Pythonic程序头，该头导入可用动作及其预期参数，显示环境对象列表，然后定义函数，这些函数的主体是在对象上操作的动作序列。这种方法通过断言前提条件的计划、例如在尝试打开冰箱之前靠近它，并通过恢复动作对失败的断言作出响应，将实际状态反馈纳入环境中。

### 方法论：

* **将机器人计划表示为Pythonic函数：** 机器人计划被表示为Pythonic程序，计划函数由动作原语的API调用、注释以总结动作和断言以跟踪执行组成。注释帮助将高级任务分解为逻辑子任务并通知LLM即将到来的动作的目标。断言提供了一个环境反馈机制，以确保前提条件得到满足，并在它们不满足时启用错误恢复。
* **构建编程语言提示：** 通过提示构建向LLM提供有关环境和原始动作的信息。这种方法确保生成的计划通常包含代理人可以采取的动作和环境中可用的对象。PROG PROMPT还包括一些示例任务—完全可执行的程序计划。

### 图表与示例：

* **图1：** 描述了PROG PROMPT如何利用LLMs在世界知识和编程语言理解方面的优势来生成可以直接执行的位于特定环境中的任务计划。
* **图2和图3：** 描述了PROG PROMPT的结构和示例，展示了如何使用Pythonic计划执行任务。

### 背景和相关工作：

背景部分讨论了机器人任务规划中的挑战以及如何在预定义的领域中进行搜索。本文还讨论了LLMs在任务规划中的作用，以及如何训练它们以展示多任务泛化。

# RT-2： Vision-Language-Action Models Transfer Web Knowledge to Robot Control

由Google DeepMind的团队撰写，发表于2023年7月28日。

### 概述：

这篇文章主要介绍了RT-2模型，这是一种利用网络知识来进行机器人控制的视觉-语言-动作模型。RT-2模型通过微调大型视觉-语言模型（VLMs），这些模型是在网络规模数据上进行训练的，直接作为可泛化和具有语义意识的机器人策略。这种方法结合了从机器人数据中学到的物理动作和从网络数据中学到的图像和文本解释能力。

### 主要内容和贡献：

* **模型结构** ：RT-2模型将机器人动作表示为另一种语言，可以转换为文本令牌，并与Internet规模的视觉-语言数据集一起训练。在推理期间，文本令牌被解令牌化成机器人动作，实现闭环控制。
* **实时推理** ：由于现代VLMs的大小可以达到数百亿参数，因此开发了一种协议，允许通过在多TPU云服务中部署模型并通过网络查询此服务来在机器人上运行RT-2模型。
* **实验** ：实验侧重于RT-2的实际应用和新能力的出现，展示了RT-2模型在对象、场景和指令的泛化方面的显著改进，并展示了从网络规模视觉-语言预训练中继承的一系列新能力。

### 实验和应用：

* **Co-Fine-Tuning** ：实验显示，与原始网络数据一起对机器人数据进行协同微调，而不是仅在机器人数据上进行微调，可以改善机器人性能。
* **应用** ：模型能够重新利用从机器人数据中学到的拾取和放置技能，将对象放置在语义指示的位置附近，例如特定的数字或图标，尽管这些线索在机器人数据中并不存在。

### 结论：

这篇文章的主要贡献是RT-2，一种模型家族，它通过微调在网络规模数据上训练的大型视觉-语言模型，直接作为可泛化和语义感知的机器人策略。实验表明，RT-2模型能够显著改善对对象、场景和指令的泛化，并展示了从网络规模视觉-语言预训练中继承的一系列新能力。

# Instruct2Act：Mapping Multi-modality Instructions to Robotic Actions with Large Language Model

### 主要目标和挑战：

文章的主要目标是开发一种类似于ChatGPT的机器人系统，该系统支持机器人操控、视觉目标达成和视觉推理。开发能够在动态环境中执行复杂任务的通用机器人系统是机器人研究中的一项重大挑战。这样的系统必须具备感知周围环境、选择相关机器人技能并相应地对其进行排序以实现长期目标的能力。

### 方法和框架：

* **Instruct2Act框架** ：该框架允许机器人基于用户的指令和由顶视图相机捕获的观察图像执行一系列动作。它旨在改变环境中的对象状态以匹配指令描述中的配置。
* **执行过程** ：框架使用LLM生成可执行代码，该代码调用视觉基础模型的API来识别环境。通过识别到的对象语义信息，我们生成可能的动作，这些动作被发送到低级控制器以执行任务。
* **无需微调** ：该框架使用视觉基础模型作为可以通过API调用的模块化工具，无需进行微调，从而消除了数据收集和训练成本的需要。

### 实验和效果：

* 文章通过在不同场景中的桌面操控领域中的机器人任务上评估它来验证了该方法的实用性和效率。
* 该方法的零射击（zero-shot）性能在多个任务中均优于许多最先进的基于学习的策略。

### 代码和应用：

* 该框架的代码已经在[GitHub](https://github.com/OpenGVLab/Instruct2Act)上公开，作为具有多模态输入的高级机器人指令任务的稳健基准。

### 结论：

该文章提出了一种新颖的框架，该框架结合了大型语言模型和多模态基础模型，以开发能够理解和执行高级指令的通用机器人操控系统。该框架具有实用性和效率，并且其代码已经公开，为相关研究提供了一个基准。

# TidyBot: Personalized Robot Assistance with Large Language Models

### 摘要：

文章探讨了如何实现个性化的机器人清理助手，该助手能够学习用户的偏好并将这些偏好应用于未来的场景。研究人员提出了一种名为TidyBot的系统，该系统能够通过捡起并整理物品来清理房间。挑战在于确定每个物品的正确位置，因为人们的偏好可能会因个人品味或文化背景而异。研究人员展示了机器人如何结合基于语言的规划和感知以及大型语言模型（LLMs）的few-shot总结能力，来推断出可广泛应用于未来互动的用户偏好。这种方法实现了快速适应，并在基准数据集上达到了91.2%的准确率。实际测试中，TidyBot成功地整理了85.0%的物品。

### 方法：

* **个性化容器选择** ：系统首先接收反映用户个人偏好的一些物品放置示例。LLM将这些示例总结为一般规则，并使用总结来确定新物品的放置位置。
* **实际部署** ：研究人员在一个实际的移动操控系统上部署了这种方法，用于家庭清理。

### 结果：

* 在基准数据集上，该方法表现出良好的泛化能力，准确率达到91.2%。
* 在实际测试场景中，TidyBot正确地整理了85.0%的物品。

### 贡献：

* 提出了文本总结与LLMs提供了机器人学中的泛化手段的观点。
* 发布了一个公开的基准数据集，用于评估容器选择偏好的泛化。
* 在实际的移动操控系统上实施和评估了该方法。

### 相关工作：

* 文章讨论了家庭清理、物品排序和LLMs在机器人学中的应用等相关工作。
* 与其他工作相比，本文展示了LLMs的总结能力如何实现机器人学中的泛化。

### 实际应用：

* 研究人员在实际的机器人移动操控系统上调查了这种方法，该系统被称为TidyBot。
* 在实际测试场景中，TidyBot成功地整理了85.0%的物品。

### 项目页面：

* 更多的补充材料、基准数据集、代码和TidyBot实际操作的定性视频可以在[项目页面](https://tidybot.cs.princeton.edu/)上找到。

# PaLM-E: An Embodied Multimodal Language Model

主要探讨了一种集成了大型语言模型（LLM）和视觉变换器（ViT）的多模态语言模型，名为PaLM-E。

### 摘要：

PaLM-E模型是一种多模态语言模型，它集成了具有562B参数的PaLM LLM和22B参数的ViT。该模型能够进行零射击（zero-shot）多模态链式思考（CoT）推理，展现了在多个任务上的多样化能力，包括视觉条件下的笑话生成、OCR免费的数学推理、多图像推理等。该模型在OK-VQA基准测试中表现出色，并且能够在机器人视觉感知、对话和计划等方面展现出多种能力。

### 主要内容和贡献：

* **模型结构** ：PaLM-E模型集成了PaLM LLM和ViT，成为目前报道中最大的视觉-语言模型。
* **多模态能力** ：该模型展现了多种能力，包括零射击多模态链式思考推理、少射击提示、OCR免费的数学推理、多图像推理等。
* **实验结果** ：PaLM-E-562B在OK-VQA基准测试中实现了最先进的性能，无需依赖任务特定的微调。
* **机器人应用** ：模型展示了在机器人视觉感知、对话和计划等方面的应用能力。
* **物理推理和空间定位** ：模型能够进行物理预测和空间定位，例如预测机器人的下一步动作和识别物体的特定特征。

### 结论：

PaLM-E模型通过多任务训练展现了在多个任务上的高效性和泛化能力，特别是在机器人任务上表现出高数据效率。该模型通过集成大型语言模型和视觉变换器，实现了一系列多模态能力，为多模态学习和机器人学提供了新的可能性和应用方向。

# RT-1: A Robotics Transformer for Vision-Language Robotic Control

主要探讨了一种名为RT-1的机器人学变换器模型，该模型用于视觉-语言机器人控制。以下是对这篇文章的详细总结：

### 摘要：

文章提出了一种名为RT-1的机器人变换器模型，该模型能够吸收大量数据并有效地推广。RT-1模型采用图像和自然语言指令作为输入，并在每个时间步为机器人输出一个动作。该模型经过大量实际训练和评估，展现了令人印象深刻的推广性、稳健性和多样性学习能力。

### 主要内容和贡献：

* **模型结构** ：RT-1模型结合了EfficientNet、TokenLearner和Transformer，以实现高效且大容量的架构。该模型能够以3Hz的频率输出动作，尽管其具有3500万个参数。
* **数据集** ：文章使用了一个大规模、多样化的数据集，该数据集包含了约130,000个示范和超过700个不同的任务指令，这些数据是在17个月的时间里通过13个机器人收集的。
* **实验结果** ：RT-1模型在训练指令上的成功率达到了97%，并且能够比下一个最佳基线更好地推广到新任务、干扰物和背景，分别提高了25%、36%和18%。
* **多任务学习** ：RT-1模型展现了在多任务学习中的高效性和泛化能力，特别是在机器人任务上表现出高数据效率。
* **实时控制** ：RT-1模型能够实现实时控制，满足实际机器人在实时环境中运行的需求。

### 方法和技术：

* **Imitation Learning** ：该模型使用模仿学习方法在演示数据集上训练策略，通过行为克隆优化π，最小化给定图像和语言指令的动作的负对数似然。
* **Action Tokenization** ：每个动作维度在RT-1中被离散化为256个bins。动作维度包括臂部运动的七个变量、基座运动的三个变量和一个用于切换三种模式的离散变量。
* **Inference Speed** ：RT-1模型具有快速且一致的推理速度，这是实际机器人在实时环境中运行的独特要求。

### 结论：

RT-1模型通过大规模、实际的训练和评估展示了显著的推广和稳健性。该模型不仅能够执行非常长期的任务，并且可以整合来自模拟或其他机器人类型的数据，保持对原始任务的性能并改善对新场景的推广。

# PROGPROMPT: Generating Situated Robot Task Plans using Large Language Models

这篇文章主要探讨了一种新的方法，名为PROGPROMPT，该方法使用大型语言模型（LLM）来生成可以在虚拟和物理环境中执行的程序。文章中详细描述了该方法的实现和评估，并展示了其在虚拟家庭环境和实际机器人上的应用。

### 主要内容：

1. **背景与相关工作：**
   * 文章提到了任务规划的背景，指出大多数机器人在预定义的领域中使用搜索进行高级规划。无条件搜索在具有许多可行动作和对象的环境中可能难以扩展。
2. **方法：**
   * PROGPROMPT方法使用了大型语言模型（如GPT-3和CODEX）来生成程序，这些程序可以在虚拟家庭环境中执行，也可以控制实际的机器人。
   * 该方法使用了特定的提示格式，包括导入语句、对象列表和示例任务，以生成计划。
   * 生成的计划可以在虚拟代理或物理机器人系统中执行，每个动作命令都会在环境中执行。
3. **实验：**
   * 文章中进行了模拟实验和真实机器人实验来评估该方法。
   * 在模拟实验中，使用了Virtual Home（VH）环境，这是一个用于典型家庭活动的确定性模拟平台。实验涉及70个家庭任务，例如“微波炉烹饪三文鱼”。
   * 在真实机器人实验中，使用了Franka-Emika Panda机器人，并实施了一个pick-and-place策略。
4. **评估与结果：**
   * 文章展示了PROGPROMPT在Virtual Home环境中生成的程序的评估结果，与其他方法相比，PROGPROMPT表现出显著的优势。
   * 文章中的表格展示了不同的LLM背景下PROGPROMPT的性能，以及与基线方法的比较。

### 结论：

这篇文章提出了一种新颖的方法，利用大型语言模型生成可执行程序，这些程序可以在虚拟环境和实际机器人中执行。实验结果表明，该方法在生成程序方面具有优越性能，表现出了实际应用的潜力

# Code-As-Policies: "Code as Policies: Language Model Programs for Embodied Control"

### 摘要：

文章提出了一种新的模型，名为“Socratic Models”，该模型能够进行零射击多模态推理。该模型结合了自然语言处理和计算机视觉，以解决需要多模态理解的问题。文章中详细描述了该模型的结构、训练方法和实验评估，并展示了其在多个任务上的应用。

### 主要内容和贡献：

* **模型结构** ：Socratic Models结合了大型语言模型和视觉模型，以实现自然语言和视觉信息的联合理解。模型能够处理自然语言指令和图像输入，并生成相应的输出。
* **零射击推理** ：该模型能够进行零射击推理，即在没有特定任务的训练数据的情况下，解决特定任务的问题。
* **多模态任务** ：文章展示了该模型在多个需要多模态理解的任务上的应用，例如视觉问答、物体检测和语言导向的任务规划等。
* **实验评估** ：文章中进行了一系列实验，以评估模型在不同任务上的性能。实验结果表明，该模型在多个基准测试中表现优异。

### 方法和技术：

* **模型训练** ：模型采用了大量的多模态数据进行训练，包括自然语言文本和图像数据。训练过程中，模型学习了语言和视觉信息的联合表示。
* **任务适应** ：模型能够适应不同的多模态任务，通过处理自然语言指令和图像输入，生成任务相关的输出。

### 结论：

Socratic Models通过结合大型语言模型和视觉模型，实现了零射击多模态推理的能力。该模型在多个需要多模态理解的任务上表现出色，展现了在实际应用中的潜力。

# **Say-Can**: "Do As I Can, Not As I Say: Grounding Language in Robotic Affordances"

### 摘要：

文章提出了一个方法，名为“SayCan”，该方法旨在从大型语言模型（LLM）中提取知识，并使机器人能够遵循高级文本指令。该方法结合了LLM的语义知识和通过强化学习获得的价值函数，以确定机器人可以执行的任务。

### 主要内容：

1. **背景** ：大型语言模型（如GPT-3、CODEX等）已经显示出了在多种任务上的广泛泛化能力。但是，如何使机器人利用这些模型的知识来执行实际任务仍然是一个挑战。
2. **SayCan方法** ：该方法结合了LLM的任务定位能力和价值函数的世界定位能力。LLM提供了任务的可能性，而价值函数提供了任务在当前状态下的可行性。
3. **实现** ：为了实现SayCan，需要为其提供一组技能。每个技kill都有一个策略、一个价值函数和一个简短的语言描述。这些技能可以通过图像为基础的行为克隆或强化学习获得。
4. **问题陈述** ：系统接收用户提供的自然语言指令，该指令描述了机器人应该执行的任务。此外，还提供了一组技能，每个技能都有一个策略、一个价值函数和一个简短的语言描述。
5. **连接大型语言模型与机器人** ：文章讨论了如何通过精心设计的提示将高级指令分解为机器人可以执行的低级指令。

### 结论：

SayCan方法结合了大型语言模型的语义知识和价值函数的物理知识，使机器人能够执行高级文本指令。实验结果表明，该方法在实际机器人任务上表现出色。

# Socratic Models: Enhancing Visual Models with Language Models for Multimodal Reasoning

主要探讨了如何通过结合视觉模型和语言模型来实现多模态推理。以下是对这篇文章的详细总结：

### 摘要：

文章提出了一种名为“Socratic Models”的方法，该方法通过结合视觉模型（VLMs）和大型语言模型（LMs）来实现多模态推理。该方法在多个任务上表现出色，包括图像字幕生成、上下文图像描述、视频到文本检索等。此外，文章还探讨了该方法在多个实际应用中的应用，如机器人感知和计划、多模态辅助对话等。

### 主要内容：

1. **方法** ：

* Socratic Models使用VLMs来生成候选字幕，并使用LMs来对这些候选字幕进行排序和优化。
* 该方法在零射击场景下表现出色，即在没有特定任务的训练数据的情况下，解决特定任务的问题。

1. **实验与结果** ：

* 在MS COCO字幕数据集上，Socratic Models在CIDEr分数上显著优于零射击基线ZeroCap，但不如直接在训练集上微调的方法，如ClipCap。
* 在Concadia数据集上，该方法在上下文图像字幕生成和描述任务上表现出色，超过了直接在训练集上微调的先前最佳方法。
* 在视频到文本检索任务上，该方法在包含长转录的视频子集上表现出色，与最佳微调方法CLIP2Video相当。

1. **应用** ：

* 文章描述了Socratic Models在多个应用中的使用，包括以自我为中心的感知、多模态辅助对话和机器人感知与计划。
* 这些应用涉及处理自然语言指令和图像输入，并生成任务相关的输出。

### 结论：

Socratic Models通过结合视觉模型和大型语言模型，实现了多模态推理的能力。该方法在多个任务和实际应用中表现出色，展现了在实际应用中的潜力。

# Can Large Language Models Reason? Extracting Actionable Knowledge from Pre-trained Models

主要探讨了大型语言模型（Large Language Models，LLMs）是否能够生成可执行的、目标驱动的行动计划。以下是对这篇文章的详细总结：

### 摘要：

文章探讨了是否可以利用LLMs中包含的知识来做出可以在交互式、实体环境中执行的、目标驱动的决策。作者使用了VirtualHome环境来模拟家庭环境中的各种实际人类活动，并通过人工评估来判断行动序列是否有意义地完成了提出的任务。研究发现，当使用单一固定的任务描述和其相关的行动序列来提示大型的GPT-3和Codex模型时，这些模型可以为我们感兴趣的任务生成非常合理的行动计划。这些完成的任务反映了模型中已经存储的信息，而且没有涉及模型微调。然而，尽管这些行动计划在语义上是正确的，但它们往往无法在环境中执行。

### 主要内容和贡献：

* **LLMs的推理能力** ：
* 大型语言模型如GPT-3和Codex能够生成看似合理的目标驱动的行动计划。
* 这些模型在没有任何额外训练的情况下，可以被提示生成这些行动计划。
* 生成的行动计划在语义上通常是正确的，但经常无法在交互式环境中执行。
* **改善模型输出的工具** ：
* 提出了几种工具来改善模型输出的可执行性，包括枚举所有可接受的行动并将模型的输出短语映射到最语义相似的可接受行动。
* 使用模型以自回归的方式生成计划中的行动，通过上述技术使过去的行动变得可接受。
* 通过使用与查询任务相似的已知任务示例来提示模型，提供弱监督。
* **实验结果** ：
* 使用上述工具可以将行动计划的可执行性从18%提高到79%，而无需对模型参数进行任何侵入性的修改。
* 但是，人类评判发现，使用上述工具生成的行动序列的正确性有所下降。

### 结论：

文章展示了，即使没有任何训练，大型语言模型也可以被提示生成看似合理的目标驱动的行动计划。然而，这些计划经常无法在交互式环境中执行。作者提出了几种工具来改善模型输出的可执行性，并通过实验验证了这些工具的有效性。

# Chat with the Environment: Interactive Multimodal Perception Using Large Language Models

### 摘要：

文章探讨了如何利用大型语言模型（LLMs）来实现交互式多模态感知，使机器人能够与环境“聊天”以获取执行任务所需的足够信息。文章提出了一个名为Matcha的交互式感知框架，该框架使用LLM作为主干，能够指导认知行为并对多模态感觉（视觉、声音、触觉、本体感觉）进行推理，以及基于交互式获取的信息来规划整个任务的执行。

### 主要内容和贡献：

* **交互式多模态感知** ：
* 机器人可以通过被动或交互式的方式感知环境。交互式感知可以帮助机器人更准确地获取信息并揭示潜在信息。
* 文章提出了一种新的交互式感知方法，使机器人能够与环境进行“聊天”并基于多模态反馈做出决策。
* **Matcha框架** ：
* Matcha是一个能够交互式感知环境的多模态环境聊天代理。
* 它包含一个LLM主干、多模态感知模块和一个低级命令执行策略，它们通过语言进行信息交换。
* LLM可以生成聊天式文本，使其能够与机器人集成，进行规划并根据环境反馈做出响应。
* **多模态学习和机器人信息收集** ：
* 多模态学习在机器人领域越来越受到关注，因为它在音频-视觉学习和语言-视觉学习等方面取得了成功。
* 机器人需要管理一个或多个传感器以最大化减少关于特定目标的歧义。
* **LLMs在机器人规划中的应用** ：
* 最近的一些工作使用LLMs将高级指令分解为可执行的低级命令进行零射击规划。
* 这些工作使机器人能够与环境互动并基于从环境中主动收集的信息做出决策。

### 结论：

本文提出了一种新的交互式多模态感知框架Matcha，它利用大型语言模型与环境进行交互式“聊天”，以获取执行任务所需的足够信息。这种方法允许机器人在多模态环境中进行高级规划和推理，并控制交互式机器人行为。

# Generative Agents: Interactive Simulacra of Human Behavior

### 摘要：

文章提出了一种名为“Generative Agents”的新型代理模型，该模型能够模拟可信的人类行为。这些代理能够执行一系列日常活动，如烹饪早餐、上班、绘画、写作等，并能够形成观点、注意到彼此并发起对话。这些代理使用大型语言模型来存储代理的经历，并能够将这些记忆随时间合成为更高级别的反思，并动态检索它们以计划行为。这些代理被用来填充一个受The Sims启发的交互式沙箱环境，其中用户可以使用自然语言与一个小镇的25个代理互动。

### 主要内容和贡献：

* **Generative Agents** ：生成剂：
* 代理能够模拟一系列可信的个人和社会行为。
* 代理能够存储其经历的完整记录，并能够合成这些记忆以计划行为。
* 代理能够在沙箱环境中与用户进行自然语言交互。
* **交互式沙箱环境** ：
* 该环境受到The Sims的启发，允许用户与一个小镇的25个代理互动。
* 代理能够执行各种活动，并能够形成观点和发起对话。
* **评估与实验** ：
* 在评估中，这些生成的代理展现出了可信的个人和社会行为。
* 代理能够记忆并反思过去的日子，并计划下一天的行为。

### 结论：

通过扩展大型语言模型来存储代理的经历、合成记忆并动态检索它们以计划行为，本文介绍了一种能够模拟可信人类行为的新型代理模型。这些代理能够在一个交互式沙箱环境中与用户进行自然语言交互，展现出了在模拟人类行为方面的潜力。

# Modeling Humans with Language Models for Interactive Robot Planning

### 摘要：

这篇文章探讨了如何使用大型语言模型（LLMs）来模拟人类，以改善人机交互（HRI）中的机器人规划。文章使用了两个HRI相关的数据集：MANNERS-DB和Trust-Transfer，以及一个人与人交互的数据集SocialIQA，来评估LLMs在模拟人类方面的性能。

### 主要内容和贡献：

* **数据集** ：
* **MANNERS-DB** ：评估机器人行为在特定社会环境中的适 appropriateness。
* **Trust-Transfer** ：研究人类对机器人能力的信任如何在观察机器人执行任务后发生变化。
* **SocialIQA** ：衡量执行社会常识推理的能力。
* **模型比较** ：
* 文章比较了DAVINCI和T5两个LLMs与各数据集的基线模型。
* 使用了错误分数（Error Score）和一致性分数（CwM Score）来评估模型的性能。
* **实验与结果** ：
* 文章进行了一系列实验来评估LLMs在HRI任务中的性能。
* 结果显示，LLMs在一些任务中表现出色，但在需要空间/物理/数值推理的HRI任务中表现不佳。
* LLMs在模拟人类方面展现出了潜力，但仍有改进空间。
* **讨论与分析** ：
* 文章分析了LLMs在HRI数据集上的表现，指出了LLMs在某些任务中的不足，并提出了可能的改进方向。
* 例如，LLMs在需要理解个人空间的任务中表现较差，可能会错误地预测机器人的行为是社会上可接受的。

### 结论：

这篇文章通过使用多个数据集和实验，探讨了大型语言模型在模拟人类以改善机器人交互规划中的应用和潜力。虽然这些模型在某些方面表现出色，但在需要特定类型推理的任务中仍存在挑战。

# Translating Natural Language to Planning Goals with Large-Language Models

这篇文章的标题是“使用大语言模型将自然语言翻译成规划目标”，由Yaqi Xie， Chen Yu， Tongyao Zhu， Jinbin Bai， Ze Gong， and Harold Soh合著。以下是对这篇文章的详细总结：

### 摘要：

本文研究了大型语言模型（LLMs）是否能够将自然语言目标翻译成结构化的规划语言（PDDL），以此作为人与计划器之间的自然界面。研究发现，LLMs更适合进行翻译而非规划。LLMs能够利用常识知识和推理来补全不完全指定的目标。然而，LLMs在涉及数值或物理（例如，空间）推理的任务中可能会失败，并且对提示敏感。

### 主要内容和贡献：

* **研究目标** ：
* 探讨LLMs是否能够将自然语言指令翻译成PDDL目标。
* 使用GPT-3.5对英语指令进行翻译，并在Blocksworld和ALFRED两个领域中展示结果。
* **实验与结果** ：
* LLMs能够生成与自然语言指令和提供的PDDL领域/问题规范一致的可实现目标。
* 即使在含糊不清的命令中，也能够得到目标。
* LLMs在简单的计数和空间推理任务中表现不佳。
* **分析** ：
* LLMs在翻译方面表现出了巨大的潜力，但在应用到一般目标翻译时还需要更多的研究。
* LLMs在理解Blocksworld领域的谓词语义和ALFRED中对象之间的层次关系方面存在失败。

### 结论：

本文通过实验表明，LLMs可以是非常有效的翻译器，能够生成与自然语言指令一致的目标。然而，LLMs在一些需要特定推理的任务中表现不佳，因此在使用这些模型时应该小心。

# PDDL PLANNING WITH PRETRAINED LARGE LANGUAGE MODELS

这篇文章似乎是关于大型语言模型（LLMs）在决策制定中的应用，特别是在规划和执行任务中的表现。以下是基于提供的片段的总结：

### 主题：

文章探讨了大型语言模型（LLMs）在解决PDDL（Planning Domain Definition Language）块世界问题中的应用，并提出块世界作为改进LLMs决策制定的基准。文章似乎比较了Codex和GPT-3两种模型，并探讨了LLMs在规划能力方面的局限性和潜力。

### 主要内容：

* 文章研究了LLMs是否能够将自然语言指令转换为可行的行动序列，并采用了一种机制来限制LLM输出到可行的行动序列。
* 文章提到了与VirtualHome相关的工作，并探讨了使用LLM嵌入和预训练技能值函数将自然语言指令映射到大量机器人任务中的技能。
* 文章提到了Valmeekam等人（2022）的工作，他们也考虑了使用LLM少量提示来解决PDDL块世界问题，并提出了块世界作为改进LLM决策制定的基准。
* 文章似乎得出了一种更乐观的观点，即尽管LLMs在某些领域可能无法自己找到完整的计划，但它们仍然可以用于指导规划。

### 结论：

尽管LLMs在某些任务中表现出色，但在需要特定类型推理的任务中仍存在挑战。文章似乎提供了一种更加乐观的观点，认为LLMs可以在某些领域指导规划，即使它们可能无法独立完成整个规划任务。

### 其他信息：

* 文章似乎是在NeurIPS 2022的Foundation Models for Decision Making (FMDM) Workshop上发表的。文章似乎是在NeurIPS 2022的Foundation Models for Decision Making （FMDM） Workshop上发表的。
* 文章中提到了多个相关工作和参考文献，涉及到大型语言模型、机器人技能、PDDL等。

# VC-1: An Artificial Visual Cortex for Embodied Intelligence

### 概述：

文章提出了对预训练的视觉表示（PVRs）或视觉“基础模型”在体现的AI中的最大和最全面的实证研究。首先，作者创建了CORTEXBENCH，它包含了17个不同的任务，涵盖了运动、导航、灵巧和移动操纵。接下来，他们系统地评估了现有的PVRs，并发现没有一个是普遍占主导地位的。为了研究预训练数据规模和多样性的影响，他们从7个不同的来源合并了超过4000小时的以自我为中心的视频，并与ImageNet结合，使用Masked Auto-Encoding（MAE）在这些数据的切片上训练不同大小的视觉变换器。与之前的工作的推断相反，他们发现扩大数据集的大小和多样性并不总是提高性能（但平均来说确实如此）。

他们的最大模型，名为VC-1，在平均上超越了所有之前的PVRs，但也并不是在所有方面都占主导地位。最后，他们展示了对VC-1进行任务或领域特定的适应可以带来显著的增益，VC-1（适应后）在CORTEXBENCH中的所有基准测试上都达到了与最佳已知结果竞争或优于的性能。

### 主要内容：

1. **背景** ：视觉被认为是生物进化的最伟大的发明之一。只有6个已知的动物门进化出了眼睛，但它们占了所有物种的95%。本文探讨了如何设计一个人工视觉皮层，这个模块在更大的计算系统中使一个人工代理能够将摄像机输入转化为动作。
2. **CORTEXBENCH** ：这是一个新的基准测试，用于评估PVRs，包括17个任务，涵盖从平面到桌面设置到真实世界室内空间的3D扫描的视觉环境。
3. **预训练的视觉表示** ：作者发现，尽管现有的PVRs通常优于从头开始的学习基线，但没有一个是普遍占主导地位的。他们发现PVRs倾向于在它们最初设计的领域（如运动、操纵、导航）中表现最好。
4. **数据集的影响** ：作者结合了来自7个不同来源的超过4000小时的以自我为中心的视频，并与ImageNet结合。他们发现，与之前的工作的推断相反，扩大数据集的大小和多样性并不总是提高性能。

### 结论：

尽管现有的PVRs在某些任务中表现出色，但在需要特定类型推理的任务中仍存在挑战。文章提供了一种更加乐观的观点，认为PVRs可以在某些领域指导规划，即使它们可能无法独立完成整个规划任务。
